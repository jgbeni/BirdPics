{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jgbeni/BirdPics.git\n",
        "!mv BirdPics/utils .\n",
        "!rm -r BirdPics\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5oajkLblM_T",
        "outputId": "6a86fd39-3dd1-4dc1-b977-3918e0f7d731"
      },
      "id": "H5oajkLblM_T",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BirdPics'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 52 (delta 16), reused 31 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (52/52), 6.76 MiB | 11.78 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "total 8\n",
            "drwxr-xr-x 1 root root 4096 Nov  4 14:36 sample_data\n",
            "drwxr-xr-x 2 root root 4096 Nov  6 11:27 utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04580ba5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04580ba5",
        "outputId": "0866709d-d9e1-4919-cecd-3aa959cbe326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import utils.data_preprocessing as dp\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/BirdPics'\n",
        "os.makedirs(dir+'/models',exist_ok=True)"
      ],
      "metadata": {
        "id": "JPe0jNTcliwL"
      },
      "id": "JPe0jNTcliwL",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "38a7101f",
      "metadata": {
        "id": "38a7101f"
      },
      "outputs": [],
      "source": [
        "f = h5py.File(dir+'/data/bird_data.hdf5', \"r\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,Y_train = f['train']['X'],np.copy(f['train']['Y'])\n",
        "X_val,Y_val = f['val']['X'],np.copy(f['val']['Y'])"
      ],
      "metadata": {
        "id": "_5K1ZQbjoTqD"
      },
      "id": "_5K1ZQbjoTqD",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dd03c10b",
      "metadata": {
        "id": "dd03c10b"
      },
      "outputs": [],
      "source": [
        "Y_train = dp.prepare_labels(Y_train)\n",
        "Y_val = dp.prepare_labels(Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dp.HDF5Dataset(X_train,Y_train,train=True)\n",
        "val_dataset = dp.HDF5Dataset(X_val,Y_val)"
      ],
      "metadata": {
        "id": "VIfZN8Tpodmz"
      },
      "id": "VIfZN8Tpodmz",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=8, batch_size=batch_size, pin_memory=True,\n",
        "                                                shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, num_workers=8, batch_size=batch_size, pin_memory=True,\n",
        "                                                shuffle=True)"
      ],
      "metadata": {
        "id": "wBfvpfufo-lu"
      },
      "id": "wBfvpfufo-lu",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model"
      ],
      "metadata": {
        "id": "BfoyRVnDHxPh"
      },
      "id": "BfoyRVnDHxPh"
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_loader)\n",
        "images,labels = next(dataiter)\n",
        "print(images.shape)\n",
        "print(labels.dtype)"
      ],
      "metadata": {
        "id": "2nzdUY7ZIKW_",
        "outputId": "325809bd-67a8-4dc4-91ec-c26580222978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2nzdUY7ZIKW_",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 224, 224])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = nn.Conv2d(3,16,3)\n",
        "pool = nn.MaxPool2d(2,2)\n",
        "conv2 = nn.Conv2d(16,32,3)\n",
        "conv3 = nn.Conv2d(32,64,3)\n",
        "conv4 = nn.Conv2d(64,64,3)\n",
        "\n",
        "x = conv1(images)\n",
        "#print(x.shape)\n",
        "x = pool(x)\n",
        "#print(x.shape)\n",
        "x = conv2(x)\n",
        "#print(x.shape)\n",
        "x = pool(x)\n",
        "#print(x.shape)\n",
        "x = conv3(x)\n",
        "#print(x.shape)\n",
        "x = pool(x)\n",
        "x = conv4(x)\n",
        "#print(x.shape)\n",
        "x = pool(x)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "1eYFiiutIRo5",
        "outputId": "f0211a40-2640-4c7e-cac4-a32cc90ab367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1eYFiiutIRo5",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64, 12, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Net(nn.Module):\n",
        "   def __init__(self, in_channels, num_classes):\n",
        "\n",
        "       \"\"\"\n",
        "       Building blocks of convolutional neural network.\n",
        "\n",
        "       Parameters:\n",
        "           * in_channels: Number of channels in the input image (for grayscale images, 1)\n",
        "           * num_classes: Number of classes to predict. In our problem, 10 (i.e digits from  0 to 9).\n",
        "       \"\"\"\n",
        "       super(CNN_Net, self).__init__()\n",
        "\n",
        "       # 1st convolutional layer\n",
        "       self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3)\n",
        "       # Max pooling layer\n",
        "       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "       # 2nd convolutional layer\n",
        "       self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "       # 3rd convolutional layer\n",
        "       self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "       # 4rd convolutional layer\n",
        "       self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "       #Dropout\n",
        "       self.dropout = nn.Dropout(0.2)\n",
        "       # Fully connected layers\n",
        "       self.fc1 = nn.Linear(128*12*12, 500)\n",
        "       self.fc2 = nn.Linear(500,num_classes)\n",
        "\n",
        "   def forward(self, x):\n",
        "       \"\"\"\n",
        "       Define the forward pass of the neural network.\n",
        "\n",
        "       Parameters:\n",
        "           x: Input tensor.\n",
        "\n",
        "       Returns:\n",
        "           torch.Tensor\n",
        "               The output tensor after passing through the network.\n",
        "       \"\"\"\n",
        "       x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\n",
        "       x = self.pool(x)           # Apply max pooling\n",
        "       x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\n",
        "       x = self.pool(x)           # Apply max pooling\n",
        "       x = F.relu(self.conv3(x))  # Apply third convolution and ReLU activation\n",
        "       x = self.pool(x)           # Apply max pooling\n",
        "       x = F.relu(self.conv4(x))  # Apply third convolution and ReLU activation\n",
        "       x = self.pool(x)           # Apply max pooling\n",
        "       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\n",
        "       x = self.dropout(x) # Dropout\n",
        "       x = F.relu(self.fc1(x))            # Apply fully connected layer 1\n",
        "       x = self.fc2(x) # Fully connected layer 2\n",
        "       return x\n",
        "model = CNN_Net(in_channels=3, num_classes=3).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "KA1bEZu1H0e_",
        "outputId": "cd98ea8a-d312-4d3c-9e6d-e9158b706cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KA1bEZu1H0e_",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc1): Linear(in_features=18432, out_features=500, bias=True)\n",
            "  (fc2): Linear(in_features=500, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Ou53fmfZJz6C"
      },
      "id": "Ou53fmfZJz6C"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pOkJhcL2vFHS"
      },
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001 ,weight_decay=1e-5)\n",
        "\n",
        "#LR decay\n",
        "decayRate = 0.96\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
      ],
      "id": "pOkJhcL2vFHS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbBgZijYwH2E",
        "outputId": "4a4e4365-cc35-4992-dcf0-7443a3362efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/312 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 312/312 [02:03<00:00,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 2.221 - train acc. 37.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss 1.051 - val acc. 50.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 312/312 [02:03<00:00,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 1.051 - train acc. 42.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss 1.039 - val acc. 51.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 312/312 [02:09<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 1.040 - train acc. 43.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss 0.993 - val acc. 55.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 159/312 [01:06<00:47,  3.19it/s]"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "num_epochs = 20\n",
        "\n",
        "max_val = 0.\n",
        "PATH = os.path.join(dir,'models/cnn_v1.pth')\n",
        "\n",
        "train_loss,val_loss = np.zeros(num_epochs,dtype=np.float32),np.zeros(num_epochs,dtype=np.float32)\n",
        "train_acc,val_acc = np.zeros(num_epochs,dtype=np.float32),np.zeros(num_epochs,dtype=np.float32)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_correct,train_samples = 0,0\n",
        "    val_correct,val_samples = 0,0\n",
        "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
        "        # origin shape: [32, 3, 224, 224] = 32, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss[epoch] += loss.item()/len(train_loader)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_samples += labels.size(0)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc[epoch] = 100.0 * train_correct / train_samples\n",
        "    print('train loss %.3f - train acc. %.2f' %(train_loss[epoch],train_acc[epoch]))\n",
        "\n",
        "\n",
        "    for val_images, val_labels in val_loader:\n",
        "        val_images = val_images.to(device)\n",
        "        val_labels = val_labels.type(torch.LongTensor)\n",
        "        val_labels = val_labels.to(device)\n",
        "        outputs = model(val_images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        val_samples += val_labels.size(0)\n",
        "        val_correct += (predicted == val_labels).sum().item()\n",
        "\n",
        "        val_loss[epoch] += criterion(outputs, val_labels).item()/len(val_loader)\n",
        "    val_acc[epoch] = 100.0 * val_correct / val_samples\n",
        "    if val_acc[epoch] > max_val:\n",
        "        max_val = val_acc[epoch]\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "    print('val loss %.3f - val acc. %.2f' %(val_loss[epoch],val_acc[epoch]))\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "print('Finished Training')"
      ],
      "id": "CbBgZijYwH2E"
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez(os.path.join(dir,'models/cnn_v1.npz'),train_loss=train_loss,val_loss=val_loss,train_acc=train_acc,val_acc=val_acc)"
      ],
      "metadata": {
        "id": "kLUEtPC9DN3d"
      },
      "id": "kLUEtPC9DN3d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}